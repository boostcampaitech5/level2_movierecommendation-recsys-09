{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/train'\n",
    "train_df = pd.read_csv(os.path.join(data_path, 'train_ratings.csv')) # 전체 학습 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>4643</td>\n",
       "      <td>1230782529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>170</td>\n",
       "      <td>1230782534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>531</td>\n",
       "      <td>1230782539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>616</td>\n",
       "      <td>1230782542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>2140</td>\n",
       "      <td>1230782563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>2722</td>\n",
       "      <td>1230782583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>2313</td>\n",
       "      <td>1230782646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>2688</td>\n",
       "      <td>1230782656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>2428</td>\n",
       "      <td>1230782694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>3113</td>\n",
       "      <td>1230782719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item        time\n",
       "0    11  4643  1230782529\n",
       "1    11   170  1230782534\n",
       "2    11   531  1230782539\n",
       "3    11   616  1230782542\n",
       "4    11  2140  1230782563\n",
       "5    11  2722  1230782583\n",
       "6    11  2313  1230782646\n",
       "7    11  2688  1230782656\n",
       "8    11  2428  1230782694\n",
       "9    11  3113  1230782719"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique train users:  31360\n",
      "Number of unique train items:  6807\n",
      "Data sparsity ratio:  0.9758536052697853\n"
     ]
    }
   ],
   "source": [
    "num_train_users = train_df['user'].nunique()\n",
    "num_train_items = train_df['item'].nunique()\n",
    "\n",
    "print (\"Number of unique train users: \", num_train_users)\n",
    "print (\"Number of unique train items: \", num_train_items)\n",
    "print(\"Data sparsity ratio: \", 1 - len(train_df) / (num_train_users * num_train_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lb = LabelEncoder()\n",
    "item_lb = LabelEncoder()\n",
    "train_df['user_idx'] = user_lb.fit_transform(train_df['user'].values)\n",
    "train_df['item_idx'] = item_lb.fit_transform(train_df['item'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>time</th>\n",
       "      <th>user_idx</th>\n",
       "      <th>item_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>4643</td>\n",
       "      <td>1230782529</td>\n",
       "      <td>0</td>\n",
       "      <td>2505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>170</td>\n",
       "      <td>1230782534</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>531</td>\n",
       "      <td>1230782539</td>\n",
       "      <td>0</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>616</td>\n",
       "      <td>1230782542</td>\n",
       "      <td>0</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>2140</td>\n",
       "      <td>1230782563</td>\n",
       "      <td>0</td>\n",
       "      <td>1183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154466</th>\n",
       "      <td>138493</td>\n",
       "      <td>44022</td>\n",
       "      <td>1260209449</td>\n",
       "      <td>31359</td>\n",
       "      <td>4882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154467</th>\n",
       "      <td>138493</td>\n",
       "      <td>4958</td>\n",
       "      <td>1260209482</td>\n",
       "      <td>31359</td>\n",
       "      <td>2652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154468</th>\n",
       "      <td>138493</td>\n",
       "      <td>68319</td>\n",
       "      <td>1260209720</td>\n",
       "      <td>31359</td>\n",
       "      <td>5768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154469</th>\n",
       "      <td>138493</td>\n",
       "      <td>40819</td>\n",
       "      <td>1260209726</td>\n",
       "      <td>31359</td>\n",
       "      <td>4791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154470</th>\n",
       "      <td>138493</td>\n",
       "      <td>27311</td>\n",
       "      <td>1260209807</td>\n",
       "      <td>31359</td>\n",
       "      <td>4363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5154471 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           user   item        time  user_idx  item_idx\n",
       "0            11   4643  1230782529         0      2505\n",
       "1            11    170  1230782534         0       109\n",
       "2            11    531  1230782539         0       319\n",
       "3            11    616  1230782542         0       368\n",
       "4            11   2140  1230782563         0      1183\n",
       "...         ...    ...         ...       ...       ...\n",
       "5154466  138493  44022  1260209449     31359      4882\n",
       "5154467  138493   4958  1260209482     31359      2652\n",
       "5154468  138493  68319  1260209720     31359      5768\n",
       "5154469  138493  40819  1260209726     31359      4791\n",
       "5154470  138493  27311  1260209807     31359      4363\n",
       "\n",
       "[5154471 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nimport pandas as pd\\nimport os\\nfrom collections import defaultdict\\nfrom sklearn.preprocessing import LabelEncoder\\n\\n\\ndef filter(df, min_user_count, min_item_count):\\n    item_count = df[\\'item\\'].nunique()\\n    user_count = df[\\'user\\'].nunique()\\n    return df, user_count, item_count\\n\\n\\ndef numerize(df, user2id):\\n    \\n    uid = list(map(lambda x: user2id[x], df[\\'uid\\']))\\n    df[\\'uid_new\\'] = uid\\n    \\n    le1 = LabelEncoder()\\n    id_lists = df[\"iid\"].unique().tolist() + [\"unknown\"]\\n    le1.fit(id_lists)\\n    df[\\'iid_new\\'] = df[\\'iid\\']\\n    iid_new = le1.transform(df[\\'iid_new\\'].astype(str))\\n    df[\\'iid_new\\'] = iid_new\\n    \\n    return df\\n\\ndef __make_user_item_interaction(config, train_df, test_df):\\n    print(\\'data preprocessing...\\')\\n\\n    df = pd.concat([train_df, test_df])\\n\\n    df = df.sort_values(by=[\"userID\", \"Timestamp\"], axis=0) \\n\\n    df.rename(columns={\\'userID\\': \\'uid\\', \\'assessmentItemID\\': \\'iid\\', \\'answerCode\\': \\'rating\\'}, inplace=True) # userID를 user로 assessmentID를 item으로 answerCode를 rating으로 생각하기 위해 컬럼명 변경 \\n\\n    df, user_count, item_count = filter(df, min_user_count=20, min_item_count=20) # 최소 사용자 수와 최소 아이템 수를 충족시키지 않은 행을 제거 후 df, 사용자 수, 아이템 수를 반환\\n                                                                                  # 일단은 20으로 설정\\n\\n    sparsity = float(df.shape[0]) / user_count.shape[0] / item_count.shape[0]\\n    print(\\'num_user: %d, num_item: %d, num_interaction: %d, sparsity: %.4f%%\\' % (user_count.shape[0], item_count.shape[0], df.shape[0], sparsity * 100))\\n\\n    unique_uid = user_count.index\\n    user2id = dict((uid, i) for (i, uid) in enumerate(unique_uid))\\n    all_df = numerize(df, user2id)\\n\\n    print(\\'data splitting...\\')\\n\\n    all_df_sorted = all_df.sort_values(by=[\\'uid_new\\', \\'Timestamp\\', \\'iid_new\\'])\\n\\n    users = np.array(all_df_sorted[\\'uid_new\\'], dtype=np.int32)\\n    items = np.array(all_df_sorted[\\'iid_new\\'], dtype=np.int32)\\n\\n    all_data = defaultdict(list) # 딕셔너리에 새로운 원소를 쉽게 추가하기 위해 defaultdict로 바꿈\\n    for n in range(len(users)):\\n        all_data[users[n]].append(items[n]) # user-item interaction dict\\n\\n    train_dict = dict()\\n\\n    for u in all_data:\\n        train_dict[u] = all_data[u][:-2]\\n\\n\\n    print(\\'preprocessed data save\\')\\n    \\n    data_dir = config[\\'data_loader\\'][\\'args\\'][\\'data_dir\\']\\n    np.save(os.path.join(data_dir, \\'preprocessed_data\\'), np.array([train_dict, max(users) + 1, max(items) + 1]))\\n    tag_df_sorted = all_df.sort_values(by=[\\'KnowledgeTag_new\\', \\'iid_new\\'])\\n    grouped_tag = tag_df_sorted.groupby(\\'KnowledgeTag_new\\').apply(lambda r: list(set(r[\\'iid_new\\'].values)))\\n    rel_dict = grouped_tag.to_dict()\\n    np.save(os.path.join(data_dir, \\'preprocessed_data_rel\\'), np.array([rel_dict]))\\n    \\n    print(\\'Making user-item interaction dict is done.\\')\\n\\n    return train_dict, rel_dict'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def filter(df, min_user_count, min_item_count):\n",
    "    item_count = df['item'].nunique()\n",
    "    user_count = df['user'].nunique()\n",
    "    return df, user_count, item_count\n",
    "\n",
    "\n",
    "def numerize(df, user2id):\n",
    "    \n",
    "    uid = list(map(lambda x: user2id[x], df['uid']))\n",
    "    df['uid_new'] = uid\n",
    "    \n",
    "    le1 = LabelEncoder()\n",
    "    id_lists = df[\"iid\"].unique().tolist() + [\"unknown\"]\n",
    "    le1.fit(id_lists)\n",
    "    df['iid_new'] = df['iid']\n",
    "    iid_new = le1.transform(df['iid_new'].astype(str))\n",
    "    df['iid_new'] = iid_new\n",
    "    \n",
    "    return df\n",
    "\n",
    "def __make_user_item_interaction(config, train_df, test_df):\n",
    "    print('data preprocessing...')\n",
    "\n",
    "    df = pd.concat([train_df, test_df])\n",
    "\n",
    "    df = df.sort_values(by=[\"userID\", \"Timestamp\"], axis=0) \n",
    "\n",
    "    df.rename(columns={'userID': 'uid', 'assessmentItemID': 'iid', 'answerCode': 'rating'}, inplace=True) # userID를 user로 assessmentID를 item으로 answerCode를 rating으로 생각하기 위해 컬럼명 변경 \n",
    "\n",
    "    df, user_count, item_count = filter(df, min_user_count=20, min_item_count=20) # 최소 사용자 수와 최소 아이템 수를 충족시키지 않은 행을 제거 후 df, 사용자 수, 아이템 수를 반환\n",
    "                                                                                  # 일단은 20으로 설정\n",
    "\n",
    "    sparsity = float(df.shape[0]) / user_count.shape[0] / item_count.shape[0]\n",
    "    print('num_user: %d, num_item: %d, num_interaction: %d, sparsity: %.4f%%' % (user_count.shape[0], item_count.shape[0], df.shape[0], sparsity * 100))\n",
    "\n",
    "    unique_uid = user_count.index\n",
    "    user2id = dict((uid, i) for (i, uid) in enumerate(unique_uid))\n",
    "    all_df = numerize(df, user2id)\n",
    "\n",
    "    print('data splitting...')\n",
    "\n",
    "    all_df_sorted = all_df.sort_values(by=['uid_new', 'Timestamp', 'iid_new'])\n",
    "\n",
    "    users = np.array(all_df_sorted['uid_new'], dtype=np.int32)\n",
    "    items = np.array(all_df_sorted['iid_new'], dtype=np.int32)\n",
    "\n",
    "    all_data = defaultdict(list) # 딕셔너리에 새로운 원소를 쉽게 추가하기 위해 defaultdict로 바꿈\n",
    "    for n in range(len(users)):\n",
    "        all_data[users[n]].append(items[n]) # user-item interaction dict\n",
    "\n",
    "    train_dict = dict()\n",
    "\n",
    "    for u in all_data:\n",
    "        train_dict[u] = all_data[u][:-2]\n",
    "\n",
    "\n",
    "    print('preprocessed data save')\n",
    "    \n",
    "    data_dir = config['data_loader']['args']['data_dir']\n",
    "    np.save(os.path.join(data_dir, 'preprocessed_data'), np.array([train_dict, max(users) + 1, max(items) + 1]))\n",
    "    tag_df_sorted = all_df.sort_values(by=['KnowledgeTag_new', 'iid_new'])\n",
    "    grouped_tag = tag_df_sorted.groupby('KnowledgeTag_new').apply(lambda r: list(set(r['iid_new'].values)))\n",
    "    rel_dict = grouped_tag.to_dict()\n",
    "    np.save(os.path.join(data_dir, 'preprocessed_data_rel'), np.array([rel_dict]))\n",
    "    \n",
    "    print('Making user-item interaction dict is done.')\n",
    "\n",
    "    return train_dict, rel_dict\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, df, train_size=0.8, train=False):\n",
    "        self.df = df\n",
    "        self.train = train\n",
    "        self.train_size = train_size\n",
    "        self.test_size = 1 - self.train_size \n",
    "\n",
    "        self.num_items = len(self.df['user_idx'].unique())\n",
    "        self.num_users = len(self.df['user_idx'].unique())\n",
    "\n",
    "        self.train_df, self.test_df = train_test_split(self.df, test_size=self.test_size, train_size=self.train_size, stratify = self.df['user_idx'].values, random_state=42)\n",
    "\n",
    "        if self.train == True:\n",
    "            self.df = self.train_df\n",
    "        else:\n",
    "            self.df = self.test_df\n",
    "\n",
    "        self.users = torch.tensor(self.df['user_idx'].values)\n",
    "        self.items = torch.tensor(self.df['item_idx'].values)\n",
    "        \n",
    "        self.interaction_mat = self.make_interaction()\n",
    "    \n",
    "    def make_interaction(self):\n",
    "        interaction_mat = np.zeros((self.num_items, self.num_users))\n",
    "        for user, item in zip(self.users, self.items):\n",
    "            interaction_mat[item][user] = 1\n",
    "\n",
    "        return interaction_mat\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.interaction_mat)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        interaction_mat = torch.tensor(self.interaction_mat[index]).float()\n",
    "\n",
    "        return interaction_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRec(nn.Module):\n",
    "    def __init__(self, num_hidden, num_users, dropout=0.05):\n",
    "        super(AutoRec, self).__init__()\n",
    "        self.encoder = nn.Linear(num_users, num_hidden)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.decoder = nn.Linear(num_hidden, num_users)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, mat):\n",
    "        hidden = self.dropout(self.sigmoid(self.encoder(mat)))\n",
    "        pred = self.decoder(hidden)\n",
    "        \n",
    "        return pred\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainer(model, train_iter, loss, optm, device):\n",
    "    model.train() # to train mode\n",
    "    loss_sum = 0\n",
    "\n",
    "    for inter_mat in train_iter:\n",
    "        inter_mat = inter_mat.to(device)    \n",
    "\n",
    "        preds = model(inter_mat)\n",
    "        loss_out = loss(preds, inter_mat)\n",
    "\n",
    "        # Update\n",
    "        optm.zero_grad() # reset gradient \n",
    "        loss_out.backward() # backpropagate\n",
    "        optm.step() # optimizer update\n",
    "        \n",
    "        loss_sum += loss_out.item()\n",
    "    \n",
    "    loss_avg = loss_sum / len(train_iter)\n",
    "\n",
    "    return loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(model, test_iter, loss, device):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_sum = 0\n",
    "\n",
    "        for inter_mat in test_iter:\n",
    "            inter_mat = inter_mat.to(device) \n",
    "\n",
    "            preds = model(inter_mat)\n",
    "            loss_out = loss(preds, inter_mat)            \n",
    "\n",
    "            loss_sum += loss_out.item()\n",
    "\n",
    "    loss_avg = loss_sum / len(test_iter)\n",
    "\n",
    "    return loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_epochs = 20\n",
    "lr = 0.002\n",
    "batch_size = 256\n",
    "\n",
    "num_users = train_df.user_idx.max() + 1\n",
    "num_hidden = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MovieLensDataset(df=train_df, train=True)\n",
    "test_dataset = MovieLensDataset(df=train_df, train=False)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) \n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoRec(num_hidden, num_users).to(device)\n",
    "loss = nn.MSELoss()\n",
    "optm = optim.Adam(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train Loss: 0.0126, test Loss: 0.0011\n",
      "epoch: 1, train Loss: 0.0067, test Loss: 0.0011\n",
      "epoch: 2, train Loss: 0.0061, test Loss: 0.0011\n",
      "epoch: 3, train Loss: 0.0055, test Loss: 0.0011\n",
      "epoch: 4, train Loss: 0.0051, test Loss: 0.0011\n",
      "epoch: 5, train Loss: 0.0047, test Loss: 0.0011\n",
      "epoch: 6, train Loss: 0.0043, test Loss: 0.0011\n",
      "epoch: 7, train Loss: 0.0041, test Loss: 0.0011\n",
      "epoch: 8, train Loss: 0.0038, test Loss: 0.0011\n",
      "epoch: 9, train Loss: 0.0037, test Loss: 0.0012\n",
      "epoch: 10, train Loss: 0.0036, test Loss: 0.0012\n",
      "epoch: 11, train Loss: 0.0035, test Loss: 0.0012\n",
      "epoch: 12, train Loss: 0.0034, test Loss: 0.0013\n",
      "epoch: 13, train Loss: 0.0034, test Loss: 0.0013\n",
      "epoch: 14, train Loss: 0.0033, test Loss: 0.0013\n",
      "epoch: 15, train Loss: 0.0033, test Loss: 0.0014\n",
      "epoch: 16, train Loss: 0.0033, test Loss: 0.0013\n",
      "epoch: 17, train Loss: 0.0033, test Loss: 0.0014\n",
      "epoch: 18, train Loss: 0.0033, test Loss: 0.0014\n",
      "epoch: 19, train Loss: 0.0032, test Loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss = trainer(model, train_dataloader, loss, optm, device)\n",
    "    test_loss = evaluator(model, test_dataloader, loss, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print(f'epoch: {epoch}, train Loss: {train_loss:.4f}, test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses,label=\"train_loss\")\n",
    "plt.plot(test_losses,label=\"test_loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_encoding(df):\n",
    "    rating_df = df.copy()\n",
    "\n",
    "    item_ids = rating_df['item'].unique()\n",
    "    user_ids = rating_df['user'].unique()\n",
    "    num_item, num_user = len(item_ids), len(user_ids)\n",
    "\n",
    "    # user, item indexing\n",
    "    item2idx = pd.Series(data=np.arange(len(item_ids)), index=item_ids) # item re-indexing (0~num_item-1)\n",
    "    \n",
    "    user2idx = pd.Series(data=np.arange(len(user_ids)), index=user_ids) # user re-indexing (0~num_user-1)\n",
    "\n",
    "    # dataframe indexing\n",
    "    rating_df = pd.merge(rating_df, pd.DataFrame({'item': item_ids, 'item_idx': item2idx[item_ids].values}), on='item', how='inner')\n",
    "    rating_df = pd.merge(rating_df, pd.DataFrame({'user': user_ids, 'user_idx': user2idx[user_ids].values}), on='user', how='inner')\n",
    "    rating_df.sort_values(['user_idx', 'time'], inplace=True)\n",
    "    del rating_df['item'], rating_df['user']\n",
    "\n",
    "    return rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split():\n",
    "    data_path = '../data/train'\n",
    "    df = pd.read_csv(os.path.join(data_path, 'train_ratings.csv')) # 전체 학습 데이터\n",
    "    df = item_encoding(df)\n",
    "\n",
    "    items = df.groupby(\"user_idx\")[\"item_idx\"].apply(list)\n",
    "    \n",
    "    # {\"user_id\" : [items]} dict\n",
    "    train_set, valid_set, item_set = {} , {}, {}\n",
    "    print(\"train_valid set split by user_idx\")\n",
    "\n",
    "    for uid, item in enumerate(tqdm(items)):\n",
    "\n",
    "        # 유저가 소비한 item의 12.5% 또는 최대 10 으로 valid_set 아이템 구성\n",
    "        # num_u_valid_set = 10\n",
    "        num_u_valid_set = min(int(len(item)*0.125), 10)\n",
    "        u_valid_set = np.random.choice(item, size=num_u_valid_set, replace=False)\n",
    "        \n",
    "        train_set[uid] = list(set(item) - set(u_valid_set))\n",
    "        valid_set[uid] = u_valid_set.tolist()\n",
    "        item_set[uid] = list(set(item))\n",
    "\n",
    "    return train_set, valid_set, item_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train valid split\n",
      "train_valid set split by user_idx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31360/31360 [00:02<00:00, 13677.82it/s]\n"
     ]
    }
   ],
   "source": [
    "print('train valid split')\n",
    "train_set, valid_set, item_set = train_valid_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_interaction(*datasets):\n",
    "    data_path = '../data/train'\n",
    "    df = pd.read_csv(os.path.join(data_path, 'train_ratings.csv')) # 전체 학습 데이터\n",
    "    df = item_encoding(df)\n",
    "\n",
    "    num_users = df['user_idx'].nunique()\n",
    "    num_items = df['item_idx'].nunique()\n",
    "\n",
    "    mat_list = []\n",
    "    dataset_list = datasets\n",
    "\n",
    "    for dataset in dataset_list: \n",
    "        inter_mat = np.zeros((num_users, num_items))\n",
    "        for uid, items in tqdm(dataset.items()):\n",
    "            for item in items:\n",
    "                inter_mat[uid][item] = 1\n",
    "        mat_list.append(inter_mat)\n",
    "\n",
    "    return mat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make csr matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31360/31360 [00:01<00:00, 19146.73it/s]\n",
      "100%|██████████| 31360/31360 [00:00<00:00, 58278.79it/s]\n",
      "100%|██████████| 31360/31360 [00:01<00:00, 17884.01it/s]\n"
     ]
    }
   ],
   "source": [
    "print('make csr matrix')\n",
    "train_mat, valid_mat, item_mat = make_interaction(train_set, valid_set, item_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class AutoRecDataset(Dataset):\n",
    "    def __init__(self, inter_mat, answers_mat):\n",
    "        #self.args = args\n",
    "        self.inter_mat = inter_mat\n",
    "        self.answers = answers_mat.argsort(axis = 1)\n",
    "\n",
    "        # valid data의 최소 길이\n",
    "        # self.answers_minlen = min([len(answer) for answer in self.answers.values()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inter_mat)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user_id = index\n",
    "        inter_mat = self.inter_mat[user_id]\n",
    "        answers = self.answers[user_id][-10:]\n",
    "       \n",
    "        cur_tensors = (\n",
    "            torch.tensor(user_id, dtype=torch.long),\n",
    "            torch.tensor(inter_mat, dtype=torch.float),\n",
    "            torch.tensor(answers, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "        return cur_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AutoRecDataset(item_mat, valid_mat)\n",
    "eval_dataset = AutoRecDataset(train_mat, valid_mat)\n",
    "submission_dataset = AutoRecDataset(item_mat, valid_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_epochs = 20\n",
    "lr = 0.002\n",
    "batch_size = 256\n",
    "\n",
    "#num_users = df.user_idx.max() + 1\n",
    "num_hidden = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle = True, pin_memory = True\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, shuffle = False, pin_memory = True\n",
    ")\n",
    "\n",
    "submission_dataloader = DataLoader(\n",
    "    submission_dataset, batch_size=batch_size, shuffle = False, pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"self.encoder = nn.Linear(num_users, num_hidden)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.decoder = nn.Linear(num_hidden, num_users)\n",
    "        self.dropout = nn.Dropout(dropout)\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoRec Model\n",
    "class AutoRec(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoRec\n",
    "    \n",
    "    Args:\n",
    "        - input_dim: (int) input feature의 Dimension\n",
    "        - emb_dim: (int) Embedding의 Dimension\n",
    "        - hidden_activation: (str) hidden layer의 activation function.\n",
    "        - out_activation: (str) output layer의 activation function.\n",
    "    Shape:\n",
    "        - Input: (torch.Tensor) input features,. Shape: (batch size, input_dim)\n",
    "        - Output: (torch.Tensor) reconstructed features. Shape: (batch size, input_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AutoRec, self).__init__()\n",
    "        \n",
    "        # initialize Class attributes\n",
    "        #self.args = args\n",
    "        \"\"\"self.input_dim = self.args.input_dim\n",
    "        self.emb_dim = self.args.hidden_size\n",
    "        self.hidden_activation = self.args.hidden_activation\n",
    "        self.out_activation = self.args.out_activation\n",
    "        self.num_layers = self.args.num_layers\n",
    "        self.dropout_rate = self.args.dropout_rate\"\"\"\n",
    "        self.input_dim = 6807 #train_mat.shape[1]\n",
    "        self.emb_dim = 64\n",
    "        self.num_layers = 1\n",
    "        self.dropout_rate = 0.05\n",
    "        \n",
    "        # define layers\n",
    "        encoder_modules = list()\n",
    "        encoder_layers = [self.input_dim] + [self.emb_dim // (2 ** i) for i in range(self.num_layers)]\n",
    "        for i in range(self.num_layers):\n",
    "            input_size = encoder_layers[i] \n",
    "            output_size = encoder_layers[i + 1] \n",
    "            encoder_modules.append(nn.Linear(input_size, output_size))\n",
    "            activation_function = nn.Sigmoid()\n",
    "            if activation_function is not None:\n",
    "                encoder_modules.append(activation_function)\n",
    "        encoder_modules.append(nn.Dropout(self.dropout_rate))\n",
    "        \n",
    "        decoder_modules = list()\n",
    "        decoder_layers = encoder_layers[::-1]\n",
    "        for i in range(self.num_layers):\n",
    "            input_size = decoder_layers[i] \n",
    "            output_size = decoder_layers[i + 1] \n",
    "            decoder_modules.append(nn.Linear(input_size, output_size))\n",
    "            activation_function = 'none'\n",
    "            if activation_function is not None and (i < self.num_layers - 1):\n",
    "                decoder_modules.append(activation_function)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *encoder_modules\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            *decoder_modules\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    # initialize weights\n",
    "    def init_weights(self):\n",
    "        for layer in self.encoder:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.xavier_normal_(layer.weight.data)\n",
    "                layer.bias.data.zero_()\n",
    "        \n",
    "        for layer in self.decoder:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.xavier_normal_(layer.weight.data)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    \n",
    "    def forward(self, input_feature):\n",
    "        h = self.encoder(input_feature)\n",
    "        output = self.decoder(h)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoRec()\n",
    "trainer = AutoRecTrainer(\n",
    "    model, train_dataloader, eval_dataloader, None, submission_dataloader, args\n",
    ")\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "from base import BaseTrainer\n",
    "from utils import inf_loop, MetricTracker\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        eval_dataloader,\n",
    "        test_dataloader,\n",
    "        submission_dataloader,\n",
    "        args,\n",
    "    ):\n",
    "\n",
    "        self.args = args\n",
    "        self.cuda_condition = torch.cuda.is_available() and not self.args.no_cuda\n",
    "        self.device = torch.device(\"cuda\" if self.cuda_condition else \"cpu\")\n",
    "\n",
    "        self.model = model\n",
    "        if self.cuda_condition:\n",
    "            self.model.cuda()\n",
    "\n",
    "        # Setting the train and test data loader\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.eval_dataloader = eval_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.submission_dataloader = submission_dataloader\n",
    "\n",
    "        # self.data_name = self.args.data_name\n",
    "        betas = (self.args.adam_beta1, self.args.adam_beta2)\n",
    "        self.optim = Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.args.lr,\n",
    "            betas=betas,\n",
    "            weight_decay=self.args.weight_decay,\n",
    "        )\n",
    "\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_dataloader)\n",
    "\n",
    "    def valid(self, epoch):\n",
    "        return self.iteration(epoch, self.eval_dataloader, mode=\"valid\")\n",
    "\n",
    "    def test(self, epoch):\n",
    "        return self.iteration(epoch, self.test_dataloader, mode=\"test\")\n",
    "\n",
    "    def submission(self, epoch):\n",
    "        return self.iteration(epoch, self.submission_dataloader, mode=\"submission\")\n",
    "\n",
    "    def iteration(self, epoch, dataloader, mode=\"train\"):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_full_sort_score(self, epoch, answers, pred_list):\n",
    "        recall, ndcg = [], []\n",
    "        for k in [5, 10]:\n",
    "            recall.append(recall_at_k(answers, pred_list, k))\n",
    "            ndcg.append(ndcg_k(answers, pred_list, k))\n",
    "        post_fix = {\n",
    "            \"Epoch\": epoch,\n",
    "            \"RECALL@5\": \"{:.4f}\".format(recall[0]),\n",
    "            \"NDCG@5\": \"{:.4f}\".format(ndcg[0]),\n",
    "            \"RECALL@10\": \"{:.4f}\".format(recall[1]),\n",
    "            \"NDCG@10\": \"{:.4f}\".format(ndcg[1]),\n",
    "        }\n",
    "        print(post_fix)\n",
    "\n",
    "        return [recall[0], ndcg[0], recall[1], ndcg[1]], str(post_fix)\n",
    "\n",
    "    def save(self, file_name):\n",
    "        torch.save(self.model.cpu().state_dict(), file_name)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def load(self, file_name):\n",
    "        self.model.load_state_dict(torch.load(file_name))\n",
    "\n",
    "    def cross_entropy(self, seq_out, pos_ids, neg_ids):\n",
    "        # [batch seq_len hidden_size]\n",
    "        pos_emb = self.model.item_embeddings(pos_ids)\n",
    "        neg_emb = self.model.item_embeddings(neg_ids)\n",
    "        # [batch*seq_len hidden_size]\n",
    "        pos = pos_emb.view(-1, pos_emb.size(2))\n",
    "        neg = neg_emb.view(-1, neg_emb.size(2))\n",
    "        seq_emb = seq_out.view(-1, self.args.hidden_size)  # [batch*seq_len hidden_size]\n",
    "        pos_logits = torch.sum(pos * seq_emb, -1)  # [batch*seq_len]\n",
    "        neg_logits = torch.sum(neg * seq_emb, -1)\n",
    "        istarget = (\n",
    "            (pos_ids > 0).view(pos_ids.size(0) * self.model.args.max_seq_length).float()\n",
    "        )  # [batch*seq_len]\n",
    "        loss = torch.sum(\n",
    "            -torch.log(torch.sigmoid(pos_logits) + 1e-24) * istarget\n",
    "            - torch.log(1 - torch.sigmoid(neg_logits) + 1e-24) * istarget\n",
    "        ) / torch.sum(istarget)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict_full(self, seq_out):\n",
    "        # [item_num hidden_size]\n",
    "        test_item_emb = self.model.item_embeddings.weight\n",
    "        # [batch hidden_size ]\n",
    "        rating_pred = torch.matmul(seq_out, test_item_emb.transpose(0, 1))\n",
    "        return rating_pred\n",
    "    \n",
    "\n",
    "class AutoRecTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        eval_dataloader,\n",
    "        test_dataloader,\n",
    "        submission_dataloader,\n",
    "        args,\n",
    "    ):\n",
    "        super(AutoRecTrainer, self).__init__(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            eval_dataloader,\n",
    "            test_dataloader,\n",
    "            submission_dataloader,\n",
    "            args,\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.MSELoss().to(self.device)\n",
    "        # self.loss_fn = nn.BCELoss().to(self.device)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optim,\n",
    "            'min',\n",
    "            factor = self.args.scheduler_factor,\n",
    "            eps = self.args.scheduler_eps,\n",
    "            patience = self.args.scheduler_patience,\n",
    "            )\n",
    "\n",
    "    def iteration(self, epoch, dataloader, mode=\"train\"):\n",
    "\n",
    "        # Setting the tqdm progress bar\n",
    "\n",
    "        rec_data_iter = tqdm.tqdm(\n",
    "            enumerate(dataloader),\n",
    "            desc=\"Recommendation EP_%s:%d\" % (mode, epoch),\n",
    "            total=len(dataloader),\n",
    "            bar_format=\"{l_bar}{r_bar}\",\n",
    "        )\n",
    "        if mode == \"train\":\n",
    "            self.model.train()\n",
    "            rec_avg_loss = 0.0\n",
    "            rec_cur_loss = 0.0\n",
    "\n",
    "            for i, batch in rec_data_iter:\n",
    "                # 0. batch_data will be sent into the device(GPU or CPU)\n",
    "                batch = tuple(t.to(self.device) for t in batch)\n",
    "                _, inter_mat, _ = batch\n",
    "\n",
    "                pred = self.model(inter_mat)\n",
    "                # pred -= pred.min(1, keepdim=True)[0]\n",
    "                # pred /= pred.max(1, keepdim=True)[0]\n",
    "                # print(pred)\n",
    "                # break\n",
    "                loss = self.loss_fn(pred, inter_mat)\n",
    "\n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "\n",
    "                rec_avg_loss += loss.item()\n",
    "                rec_cur_loss = loss.item()\n",
    "\n",
    "\n",
    "            rec_avg_loss /= len(rec_data_iter)\n",
    "\n",
    "            self.scheduler.step(rec_avg_loss)\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"rec_avg_loss\": \"{:.4f}\".format(rec_avg_loss),\n",
    "                \"rec_cur_loss\": \"{:.4f}\".format(rec_cur_loss),\n",
    "            }\n",
    "\n",
    "            if (epoch + 1) % self.args.log_freq == 0:\n",
    "                print(str(post_fix))\n",
    "\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "            pred_list = None\n",
    "            answer_list = None\n",
    "            with torch.no_grad():\n",
    "                for i, batch in rec_data_iter:\n",
    "                    batch = tuple(t.to(self.device) for t in batch)\n",
    "                    user_ids, inter_mat, answers = batch\n",
    "\n",
    "                    rating_pred = self.model(inter_mat)\n",
    "                    # rating_pred = rating_pred.softmax(dim = 1)\n",
    "                    # print(rating_pred)\n",
    "                    # break\n",
    "\n",
    "                    rating_pred = rating_pred.cpu().data.numpy().copy()\n",
    "                    batch_user_index = user_ids.cpu().numpy()\n",
    "                    rating_pred[self.args.train_matrix[batch_user_index] > 0] = -1\n",
    "\n",
    "                    ind = np.argpartition(rating_pred, -10)[:, -10:]\n",
    "                    # ind = np.argpartition(rating_pred, -20)[:, -20:]\n",
    "\n",
    "                    arr_ind = rating_pred[np.arange(len(rating_pred))[:, None], ind]\n",
    "\n",
    "                    arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(rating_pred)), ::-1]\n",
    "\n",
    "                    batch_pred_list = ind[\n",
    "                        np.arange(len(rating_pred))[:, None], arr_ind_argsort\n",
    "                    ]\n",
    "\n",
    "                    if i == 0:\n",
    "                        pred_list = batch_pred_list\n",
    "                        answer_list = answers.cpu().data.numpy()\n",
    "                    else:\n",
    "                        pred_list = np.append(pred_list, batch_pred_list, axis=0)\n",
    "                        answer_list = np.append(\n",
    "                            answer_list, answers.cpu().data.numpy(), axis=0\n",
    "                        )\n",
    "\n",
    "            if mode == \"submission\":\n",
    "                return pred_list\n",
    "            else:\n",
    "                return self.get_full_sort_score(epoch, answer_list, pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
